{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler, StratifiedKFold, KFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.max_rows = 500\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import io, math, requests\n",
    "\n",
    "# Only works in Python3, see comment below for Python2\n",
    "def submit_prediction(df, sep=',', **kwargs):\n",
    "    # TOKEN to recover on the platform: \"Submissions\"> \"Submit from your Python Notebook\"\n",
    "    TOKEN='2b828abeb51e873238ac39f3d3f2f9d4fcac34a17c1fbd07fb899f3f2c60171e665fd7c3d553a8090855a8442b14c0033a2e8bb46500f5f2aa7393319c78a627'  \n",
    "    URL='https://qscore.datascience-olympics.com/api/submissions'\n",
    "    buffer = io.BytesIO() # Python 2\n",
    "    #buffer = io.StringIO() # Python 3\n",
    "    df.to_csv(buffer, sep=sep, **kwargs)\n",
    "    buffer.seek(0)\n",
    "    r = requests.post(URL, headers={'Authorization': 'Bearer {}'.format(TOKEN)},files={'datafile': buffer})\n",
    "    if r.status_code == 429:\n",
    "        raise Exception('Submissions are too close. Next submission is only allowed in {} seconds.'.format(int(math.ceil(int(r.headers['x-rate-limit-remaining']) / 1000.0))))\n",
    "    if r.status_code != 200:\n",
    "        raise Exception(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "data_path = '../../Data'\n",
    "submission_path = '../../submission'\n",
    "train = pd.read_csv(os.path.join(data_path, \"X_train_prep.csv\"))\n",
    "test = pd.read_csv(os.path.join(data_path, \"X_test_prep.csv\"))\n",
    "target = pd.read_csv(os.path.join(data_path, \"y_train.csv\"), index_col=0).values.flatten()\n",
    "cat_cols = pd.read_csv(os.path.join(data_path, \"cat_cols.csv\"), header=-1)\n",
    "cat_cols = list(cat_cols.T.values.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def get_sample(train, target, rate=0.05):\n",
    "    np.random.seed(0)\n",
    "    r = np.random.choice([True, False], len(train), p=[rate, 1-rate])\n",
    "    r.sum()\n",
    "    train = train[r]\n",
    "    target = target[r]\n",
    "    return train, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, target = get_sample(train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "lTrain = lgb.Dataset(train, label=target, categorical_feature=cat_cols, free_raw_data=False)\n",
    "lTest = lgb.Dataset(test, categorical_feature=cat_cols, free_raw_data=False)\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "problematic = 'multiclass' # binary, regression, multiclass\n",
    "metric = 'multi_logloss' # logloss, l1, l2, l2_root, binary_logloss, binary_error, auc, multi_error, multi_logloss\n",
    "sklearn_metric = 'log_loss' # roc_auc_score, accuracy_score, mean_absolute_error, mean_squared_error, log_loss\n",
    "n_classes = 3\n",
    "random_seed = 0\n",
    "n_folds = 5\n",
    "n_iter = 30\n",
    "submit = True\n",
    "to_csv = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Set params\n",
    "params = {'boosting_type': 'gbdt',\n",
    "          'max_depth' : -1,\n",
    "          'objective': problematic,\n",
    "          'metric' : metric,\n",
    "          'nthread': -1,\n",
    "          'n_estimators': 100000,\n",
    "          'num_leaves': 128,\n",
    "          'learning_rate': 0.05,\n",
    "          'max_bin': 512,\n",
    "          'subsample': 1,\n",
    "          'subsample_freq': 1,\n",
    "          'colsample_bytree': 0.8,\n",
    "          'reg_alpha': 5,\n",
    "          'reg_lambda': 10,\n",
    "          #'min_split_gain': 0.5,\n",
    "          #'min_child_weight': 1,\n",
    "          #'min_child_samples': 5,\n",
    "          #'scale_pos_weight': 1,\n",
    "          #'subsample_for_bin': 200,\n",
    "          #'reg_sqrt': True, # for regression\n",
    "          'random_state': random_seed}\n",
    "\n",
    "if problematic == 'multiclass':\n",
    "    params['num_class'] = n_classes\n",
    "\n",
    "# Create parameters to search\n",
    "gridParams = {\n",
    "    'learning_rate': [0.01, 0.005, 0.001],\n",
    "    'num_leaves' : [2**4, 2**5, 2**6, 2**7, 2**8],\n",
    "    'colsample_bytree' : [0.4, 0.6, 0.8],\n",
    "    'subsample' : [0.4, 0.6, 0.8],\n",
    "    'reg_alpha' : [0.01, 0.1, 1],\n",
    "    'reg_lambda' : [0.01, 0.1, 1]\n",
    "    }\n",
    "\n",
    "param_list = list(ParameterSampler(gridParams, n_iter=n_iter, random_state=random_seed))\n",
    "\n",
    "grid_search_params = []\n",
    "for param in param_list:\n",
    "    params.update(param)\n",
    "    grid_search_params.append(params.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Find best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "stratified = True\n",
    "if problematic == 'regression':\n",
    "    stratified = False\n",
    "\n",
    "best_scores = {}\n",
    "for i, param in enumerate(grid_search_params):\n",
    "    model = lgb.cv(param, lTrain, nfold=n_folds, verbose_eval=200,\n",
    "                   early_stopping_rounds=200, stratified=stratified)\n",
    "    res = {'best_score': min(model[metric + '-mean'])}\n",
    "    res.update(param.copy())\n",
    "    res['params'] = param.copy()\n",
    "    best_scores['model_{}'.format(i)] = res.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search_results = pd.DataFrame(best_scores).T.sort_values('best_score')\n",
    "best_params = grid_search_results['params'][0]\n",
    "grid_search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Re-trained top 3 best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def train_and_submit(params, train, target, test, n_folds, n_classes, cat_cols,\n",
    "                     metric, random_seed=0, submit=True, to_csv=True):\n",
    "\n",
    "    if params['objective'] == 'regression':\n",
    "        folds = KFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n",
    "    else:\n",
    "        folds = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n",
    "\n",
    "    if params['objective'] == 'multiclass':\n",
    "        oof = np.zeros((len(train), n_classes))\n",
    "        predictions = np.zeros((len(test), n_classes))\n",
    "    else:\n",
    "        oof = np.zeros(len(train))\n",
    "        predictions = np.zeros(len(test))\n",
    "    feature_importance = pd.DataFrame(np.zeros((train.shape[1], n_folds)), index=train.columns)\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target)):\n",
    "        print(\"Fold {}\".format(fold_))\n",
    "        trn_data = lgb.Dataset(train.iloc[trn_idx], label=target[trn_idx],\n",
    "                               categorical_feature=cat_cols, free_raw_data=False)\n",
    "        val_data = lgb.Dataset(train.iloc[val_idx], label=target[val_idx],\n",
    "                               categorical_feature=cat_cols, free_raw_data=False)\n",
    "        clf = lgb.train(best_params, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 200)\n",
    "        oof[val_idx] = clf.predict(train.iloc[val_idx], num_iteration=clf.best_iteration)\n",
    "        predictions += clf.predict(test, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "        feature_importance.iloc[:, fold_] = clf.feature_importance()\n",
    "    \n",
    "    allowed_metric = ['log_loss', 'roc_auc_score', 'accuracy_score', 'mean_absolute_error', 'mean_squared_error']\n",
    "    if metric not in allowed_metric:\n",
    "        raise('Not allowed metric')\n",
    "    f_score = eval(metric)    \n",
    "    score = f_score(target, oof)\n",
    "    print(\"CV score: {:<8.5f}\".format(score))\n",
    "    df_submission = pd.DataFrame(predictions, index=test.index)\n",
    "    \n",
    "    feature_importance = feature_importance.mean(axis=1).sort_values(ascending=False)\n",
    "\n",
    "    if submit:\n",
    "        submit_prediction(df_submission, sep=',', index=True)\n",
    "    if to_csv:\n",
    "        df_submission.to_csv(os.path.join(submission_path, \"submission_{}.csv\".format(score)), index=False)\n",
    "\n",
    "    return feature_importance, oof, predictions, df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "best_model = train_and_submit(params=best_params,\n",
    "                              train=train,\n",
    "                              target=target,\n",
    "                              test=test,\n",
    "                              n_folds=n_folds,\n",
    "                              n_classes=n_classes,\n",
    "                              cat_cols=cat_cols,\n",
    "                              metric=sklearn_metric,\n",
    "                              random_seed=random_seed,\n",
    "                              submit=submit,\n",
    "                              to_csv=to_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "second_model = train_and_submit(params=grid_search_results['params'][1],\n",
    "                                train=train,\n",
    "                                target=target,\n",
    "                                test=test,\n",
    "                                n_folds=n_folds,\n",
    "                                n_classes=n_classes,\n",
    "                                cat_cols=cat_cols,\n",
    "                                metric=sklearn_metric,\n",
    "                                random_seed=random_seed,\n",
    "                                submit=submit,\n",
    "                                to_csv=to_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "third_model = train_and_submit(params=grid_search_results['params'][2],\n",
    "                               train=train,\n",
    "                               target=target,\n",
    "                               test=test,\n",
    "                               n_folds=n_folds,\n",
    "                               n_classes=n_classes,\n",
    "                               cat_cols=cat_cols,\n",
    "                               metric=sklearn_metric,\n",
    "                               random_seed=random_seed,\n",
    "                               submit=submit,\n",
    "                               to_csv=to_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imp = best_model[0]\n",
    "imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Retrain with feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "col_to_keep = list(train.loc[:, imp != 0].columns)\n",
    "col_to_drop = list(train.loc[:, imp == 0].columns)\n",
    "drop_cat = [elem for elem in set(col_to_drop).intersection(cat_cols)]\n",
    "new_cat_cols = [x for x in cat_cols if x not in drop_cat]\n",
    "print(drop_cat)\n",
    "pd.Series(col_to_keep).to_csv(os.path.join(data_path, \"selected_features.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "best_model_feature_selection = train_and_submit(params=best_params,\n",
    "                                                train=train.loc[:, col_to_keep],\n",
    "                                                target=target,\n",
    "                                                test=test.loc[:, col_to_keep],\n",
    "                                                n_folds=n_folds,\n",
    "                                                n_classes=n_classes,\n",
    "                                                cat_cols=new_cat_cols,\n",
    "                                                metric=sklearn_metric,\n",
    "                                                random_seed=random_seed,\n",
    "                                                submit=submit,\n",
    "                                                to_csv=to_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Retrain without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "train_out = pd.read_csv(os.path.join(data_path, \"X_train_prep_without_out.csv\"))\n",
    "target_out = pd.read_csv(os.path.join(data_path, \"y_train_without_out.csv\"), header=-1).values.flatten()\n",
    "\n",
    "#train_out, target_out = get_sample(train_out, target_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "best_model_feature_selection_without_outliers = train_and_submit(params=best_params,\n",
    "                                                                 train=train_out.loc[:, col_to_keep],\n",
    "                                                                 target=target_out,\n",
    "                                                                 test=test.loc[:, col_to_keep],\n",
    "                                                                 n_folds=n_folds,\n",
    "                                                                 n_classes=n_classes,\n",
    "                                                                 cat_cols=new_cat_cols,\n",
    "                                                                 metric=sklearn_metric,\n",
    "                                                                 random_seed=random_seed,\n",
    "                                                                 submit=submit,\n",
    "                                                                 to_csv=to_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_csv = True\n",
    "submit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "if problematic=='multiclass':\n",
    "    submission_files = os.listdir(submission_path)\n",
    "    stacking = pd.DataFrame(np.zeros((ntest, n_classes)))\n",
    "\n",
    "    for file_name in submission_files:\n",
    "        if file_name.startswith('submission_'):\n",
    "            submission = pd.read_csv(os.path.join(submission_path, file_name))\n",
    "            submission.columns = stacking.columns.copy()\n",
    "            stacking = stacking.add(submission/len(submission_files))\n",
    "\n",
    "    if submit:\n",
    "        submit_prediction(stacking, sep=',', index=True)\n",
    "    if to_csv:\n",
    "        stacking.to_csv(os.path.join(submission_path, \"stacking.csv\"), index=False)\n",
    "        \n",
    "        \n",
    "else:\n",
    "    models = ['best_model', 'second_model', 'third_model', 'best_model_feature_selection']\n",
    "\n",
    "    stacking = pd.DataFrame(np.zeros(ntest))\n",
    "\n",
    "    for model in models:\n",
    "        score = pd.DataFrame(eval(model)[2])\n",
    "        stacking = stacking + score/len(models)\n",
    "\n",
    "    if submit:\n",
    "        submit_prediction(stacking, sep=',', index=True)\n",
    "    if to_csv:\n",
    "        stacking.to_csv(os.path.join(submission_path, \"stacking.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "models = ['best_model', 'second_model', 'third_model', 'best_model_feature_selection']\n",
    "\n",
    "CV_scores = pd.DataFrame(np.zeros((ntrain, len(models))), columns=models)\n",
    "test_scores = pd.DataFrame(np.zeros((ntest, len(models))), columns=models)\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    CV_scores.iloc[:, i] = eval(model)[1]\n",
    "    test_scores.iloc[:, i] = eval(model)[2]\n",
    "\n",
    "if problematic=='binary':\n",
    "    clf = RandomForestClassifier(random_state=random_seed, n_estimators=100, max_depth=10)\n",
    "    clf.fit(CV_scores.values, target)\n",
    "    preds = clf.predict_proba(CV_scores.values)[:, 1]\n",
    "    f_score = eval(sklearn_metric)\n",
    "    score = f_score(target, preds)\n",
    "    print(\"Blend score: {:<8.5f}\".format(score))\n",
    "    estimates = clf.predict_proba(test_scores.values)[:, 1]\n",
    "    blending = pd.DataFrame(estimates, index=test.index)\n",
    "    if submit:\n",
    "        submit_prediction(blending, sep=',', index=True)\n",
    "    if to_csv:\n",
    "        blending.to_csv(os.path.join(submission_path, \"blending_{}.csv\".format(score)), index=False)\n",
    "\n",
    "elif problematic=='regression':\n",
    "    clf = RandomForestRegressor(random_state=random_seed, n_estimators=100, max_depth=10)\n",
    "    clf.fit(CV_scores.values, target)\n",
    "    preds = clf.predict(CV_scores.values)\n",
    "    f_score = eval(sklearn_metric)\n",
    "    score = f_score(target, preds)\n",
    "    print(\"Blend score: {:<8.5f}\".format(score))\n",
    "    estimates = clf.predict(test_scores.values)\n",
    "    blending = pd.DataFrame(estimates, index=test.index)\n",
    "    if submit:\n",
    "        submit_prediction(blending, sep=',', index=True)\n",
    "    if to_csv:\n",
    "        blending.to_csv(os.path.join(submission_path, \"blending_{}.csv\".format(score)), index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": ".Olympics",
   "language": "python",
   "name": ".olympics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
