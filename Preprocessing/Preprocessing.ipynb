{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import pprint\n",
    "import pickle\n",
    "from itertools import chain\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run preprocessing_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../Data'\n",
    "training_name = 'train_requests.csv'\n",
    "test_name = 'test_requests.csv'\n",
    "target_name = 'granted_number_of_nights'\n",
    "id_name = 'request_id' # None\n",
    "date_features = ['answer_creation_date', 'group_creation_date', 'request_creation_date'] # None\n",
    "detect_sample_size = 5000\n",
    "create_csv_target = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and detect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_sample = pd.read_csv(os.path.join(data_path, training_name),\n",
    "                           sep=',', nrows=detect_sample_size, index_col=id_name)\n",
    "train_sample.drop([target_name], inplace=True, axis=1)\n",
    "test_sample = pd.read_csv(os.path.join(data_path, training_name),\n",
    "                          sep=',', nrows=detect_sample_size, index_col=id_name)\n",
    "train_test_sample = pd.concat((train_sample, test_sample), axis=0, sort=True)\n",
    "column_dtypes = optimize_csv(train_test_sample, date_features)\n",
    "\n",
    "print('\\nDetected types : \\n')\n",
    "pprint.pprint(column_dtypes)\n",
    "print('-'*20)\n",
    "\n",
    "del train_sample, test_sample\n",
    "\n",
    "t0 = time.time()\n",
    "# Train dataset\n",
    "# dateparser = lambda x: pd.to_datetime(x, format='%d%b%Y:%H:%M:%S')\n",
    "train = pd.read_csv(os.path.join(data_path, training_name), sep=',', dtype=column_dtypes,\n",
    "                    index_col=id_name, parse_dates=date_features or False,\n",
    "                    infer_datetime_format=True,\n",
    "                    error_bad_lines=False)\n",
    "t1 = time.time()\n",
    "# Test dataset\n",
    "# dateparser = lambda x: pd.to_datetime(x, format='%d%b%Y:%H:%M:%S')\n",
    "test = pd.read_csv(os.path.join(data_path, test_name), sep=',', dtype=column_dtypes,\n",
    "                   index_col=id_name, parse_dates=date_features or False,\n",
    "                   infer_datetime_format=True,\n",
    "                   error_bad_lines=False)\n",
    "t2 = time.time()\n",
    "\n",
    "target = train[target_name]\n",
    "if create_csv_target:\n",
    "    pd.DataFrame(target).to_csv(os.path.join(data_path, \"y_train.csv\"), index=False)\n",
    "    print('\\ny_train.csv have been created with the target')\n",
    "    print('-'*20)\n",
    "\n",
    "train.drop([target_name], inplace=True, axis=1)\n",
    "print(f'\\n{target_name} have been dropped into the training set')\n",
    "print('-'*20)\n",
    "# requests.to_csv(os.path.join(data_path, \"X_train.csv\"), index=False)\n",
    "# requests_test.to_csv(os.path.join(data_path, \"X_test.csv\"), index=False)\n",
    "\n",
    "duplicated = train.duplicated().sum()\n",
    "if duplicated > 0:\n",
    "    print('\\nNb of duplicated row : ' + str(duplicated))\n",
    "    print('-'*20)\n",
    "    \n",
    "print(f'\\nTime to import {training_name} : {np.round(t1 - t0, 2)}s')\n",
    "print('-'*20)\n",
    "print(f'Time to import {test_name} : {np.round(t2 - t1, 2)}s')\n",
    "print('-'*20)\n",
    "print(f'Shape of {training_name} : {train.shape}')\n",
    "print('-'*20)\n",
    "print(f'Shape of {test_name} : {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = train.shape[0]\n",
    "train_test = pd.concat((train, test), axis=0)\n",
    "del train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if date_features:\n",
    "    for col in date_features:\n",
    "        print('-'*20)\n",
    "        print(col)\n",
    "        print('-'*20)\n",
    "        print(train_test[col].min())\n",
    "        print(train_test[col].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fillna_value_num = -1\n",
    "fillna_value_cat = \"MiSsInG\"\n",
    "fillna_value_date = 0\n",
    "\n",
    "cat_features = train_test.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "num_features = train_test.select_dtypes(include=[np.number]).columns.tolist()\n",
    "date_features = train_test.select_dtypes(include=['datetime']).columns.tolist()\n",
    "\n",
    "fillna_dict = {}\n",
    "for col in train_test.columns:\n",
    "    if col in num_features:\n",
    "        fillna_dict[col] = fillna_value_num\n",
    "    elif col in date_features:\n",
    "        fillna_dict[col] = fillna_value_date\n",
    "    else:\n",
    "        if train_test[col].dtype.name == 'category':\n",
    "            train_test[col] = train_test[col].cat.add_categories([fillna_value_cat])\n",
    "        fillna_dict[col] = fillna_value_cat\n",
    "\n",
    "train_test['nb_nan'] = train_test.isnull().sum(axis=1)\n",
    "train_test.fillna(value=fillna_dict, inplace=True)\n",
    "\n",
    "assert train_test.isnull().sum().sum() == 0, 'There are missing values in the dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distinct count\n",
    "agg_features = {'social_situation_id': 'group_id',\n",
    "                'town': 'group_id',\n",
    "                'victim_of_violence': 'group_id',\n",
    "                'victim_of_violence_type': 'group_id',\n",
    "                'requester_type': 'group_id',\n",
    "                'request_backoffice_creator_id': 'group_id',\n",
    "                'number_of_underage': 'group_id',\n",
    "                'long_term_housing_request': 'group_id',\n",
    "                'housing_situation_label': 'group_id',\n",
    "                'group_type': 'group_id',\n",
    "                'district': 'group_id',\n",
    "                'child_to_come': 'group_id',\n",
    "                'child_situation': 'group_id',\n",
    "                'animal_presence': 'group_id'}\n",
    "\n",
    "params = [(train_test[[by_field]+[field]].copy(),field, by_field) for\n",
    "          field, by_field in agg_features.items()]\n",
    "\n",
    "# Min and max key by value\n",
    "agg_features2 = {'answer_creation_date': 'group_id',\n",
    "                 'request_creation_date': 'group_id',\n",
    "                 'number_of_underage': 'group_id'}\n",
    "\n",
    "params2 = [(train_test[[by_field]+[field]].copy(),field, by_field) for\n",
    "           field, by_field in agg_features2.items()]\n",
    "\n",
    "# Most freq key by value\n",
    "agg_features3 = {'answer_creation_date': 'group_id',\n",
    "                 'request_creation_date': 'group_id',\n",
    "                 'number_of_underage': 'group_id'}\n",
    "\n",
    "params3 = [(train_test[[by_field]+[field]].copy(),field, by_field) for\n",
    "           field, by_field in agg_features3.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new_features_distinct_count = do_parallel(get_distinct_count, params)\n",
    "new_features_min = do_parallel(get_min, params2)\n",
    "new_features_max = do_parallel(get_max, params2)\n",
    "new_features_mode = do_parallel(get_mode, params3)\n",
    "\n",
    "for col in chain(new_features_distinct_count,\n",
    "                 new_features_min,\n",
    "                 new_features_max,\n",
    "                 new_features_mode):\n",
    "    train_test = pd.concat([train_test, col], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_bool, days_bool, month_bool, year_bool = True, True, True, True\n",
    "date_cols = train_test.select_dtypes(np.datetime64).columns\n",
    "\n",
    "params_date = [(train_test[date_col].copy(),hour_bool, days_bool, month_bool, year_bool) for date_col in date_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new_features_date = do_parallel(get_date_features, params_date)\n",
    "\n",
    "for col in new_features_date:\n",
    "    train_test = pd.concat([train_test, col], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test.drop(date_cols, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key: text feature, value: specific tokeniser (separator)\n",
    "text_feature = {'housing_situation_label': None, # custom_tokenizer,\n",
    "                'group_composition_label': None}\n",
    "\n",
    "language = 'english' # 'french'\n",
    "\n",
    "# Text features\n",
    "params_text = [(train_test[text_col].copy(), language) for text_col in text_feature.keys()]\n",
    "\n",
    "# Tfidf features\n",
    "max_features = 5\n",
    "params_tfidf = [(train_test[text_col].copy(), tokenizer, language, max_features) for\n",
    "                text_col, tokenizer in text_feature.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new_features_text = do_parallel(get_text_feature, params_text)\n",
    "new_features_tfidf = do_parallel(get_tfidf_vectorizer, params_tfidf)\n",
    "\n",
    "for col in chain(new_features_text, new_features_tfidf):\n",
    "    train_test = pd.concat([train_test, col], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test.drop(text_feature.keys(), inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop = ['group_id', 'group_main_requester_id', 'housing_situation_id', 'group_composition_id']\n",
    "col_to_drop.extend(list(date_cols))\n",
    "# delete col_to_drop and date features\n",
    "train_test.drop(col_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_test.iloc[:ntrain, :]\n",
    "test = train_test.iloc[ntrain:, :]\n",
    "del train_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_features = train.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "params_num_features = [(train[num_feature].values, test[num_feature].values) for num_feature in num_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preproc_num_features = do_parallel(standard_scaler, params_num_features)\n",
    "\n",
    "for col_name, preproc_num_feature in zip(num_features, preproc_num_features):\n",
    "    train.loc[:, col_name] = preproc_num_feature[0]\n",
    "    test.loc[:, col_name] = preproc_num_feature[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = train.select_dtypes(include=['object', 'category']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modality grouper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_grouper = 50\n",
    "\n",
    "params_modality_grouper = [(train[cat_feature].values,\n",
    "                            test[cat_feature].values,\n",
    "                            thresh_grouper) for cat_feature in cat_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preproc_cat_features = do_parallel(modality_grouper, params_modality_grouper)\n",
    "\n",
    "for col_name, preproc_cat_feature in zip(cat_features, preproc_cat_features):\n",
    "    train.loc[:, col_name] = preproc_cat_feature[0]\n",
    "    test.loc[:, col_name] = preproc_cat_feature[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_target_encoder = [(train[cat_feature].values,\n",
    "                          test[cat_feature].values,\n",
    "                          target.values) for cat_feature in cat_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preproc_cat_features = do_parallel(target_encoder, params_target_encoder)\n",
    "\n",
    "for col_name, preproc_cat_feature in zip(cat_features, preproc_cat_features):\n",
    "    train.loc[:, col_name + '_target_enc'] = preproc_cat_feature[0]\n",
    "    test.loc[:, col_name + '_target_enc'] = preproc_cat_feature[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One  hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_limit = 10\n",
    "one_hot_features = [col for col in cat_features if train[col].nunique() <= 10]\n",
    "params_one_hot_encoder = [(train[cat_feature],\n",
    "                           test[cat_feature],\n",
    "                           cat_feature) for cat_feature in one_hot_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preproc_cat_features = do_parallel(one_hot_encoder, params_one_hot_encoder)\n",
    "\n",
    "for col in preproc_cat_features:\n",
    "    train = pd.concat([train, col[0]], axis=1, sort=False)\n",
    "    test = pd.concat([test, col[1]], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinal encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_ordinal_encoder = [(train[cat_feature].values,\n",
    "                           test[cat_feature].values) for cat_feature in cat_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preproc_cat_features = do_parallel(ordinal_encoder, params_ordinal_encoder)\n",
    "\n",
    "for col_name, preproc_cat_feature in zip(cat_features, preproc_cat_features):\n",
    "    train.loc[:, col_name] = preproc_cat_feature[0]\n",
    "    test.loc[:, col_name] = preproc_cat_feature[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete constant cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_constant = list(train.loc[:, train.nunique() == 1].columns)\n",
    "print('constant columns: ')\n",
    "print(col_constant)\n",
    "train.drop(col_constant, axis=1, inplace=True)\n",
    "test.drop(col_constant, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "train_test = pd.concat([train, test], axis=0)\n",
    "for col in train_test.columns:\n",
    "    le = LabelEncoder()\n",
    "    train_test[col] = le.fit_transform(train_test[col])\n",
    "    \n",
    "out = IsolationForest(n_estimators=300, max_samples=0.1, max_features=0.7, bootstrap=True,\n",
    "                          n_jobs=-1, random_state=0, contamination='auto', behaviour='new')\n",
    "train_test_outliers = out.fit_predict(train_test)\n",
    "train['is_outlier'] = train_test_outliers[:ntrain]\n",
    "test['is_outlier'] = train_test_outliers[ntrain:]\n",
    "\n",
    "nb_outliers_train = train['is_outlier'].sum()\n",
    "nb_outliers_test = test['is_outlier'].sum()\n",
    "\n",
    "print('outliers training set: {} ({} %)'.format(nb_outliers_train,\n",
    "                                                float(nb_outliers_train)/train.shape[0]))\n",
    "print('outliers test set: {} ({} %)'.format(nb_outliers_test,\n",
    "                                            float(nb_outliers_test)/test.shape[0]))\n",
    "del train_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(train.shape[0] == ntrain)\n",
    "assert(train.shape[1] == test.shape[1])\n",
    "assert((set(train.columns) - set(test.columns)) == set())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(os.path.join(data_path, \"X_train_prep.csv\"), index=True)\n",
    "test.to_csv(os.path.join(data_path, \"X_test_prep.csv\"), index=True)\n",
    "pd.Series(cat_features).to_csv(os.path.join(data_path, \"cat_cols.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
